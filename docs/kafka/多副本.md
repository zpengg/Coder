# 多副本 
一个分区有一或多个副本
多副本保证 [[/分布式/高可用]]
单独分析数据[[可靠性]]

## 设计理念
集群一致性 常适用多数表决
数据一致性 用其他经济手段解决
kafka 使用 ISR集合 选举（随机）

> Our protocol for allowing a replica to rejoin the ISR ensures that before rejoining, it must fully re-sync again even if it lost unflushed data in its crash.

> http://kafka.apache.org/26/documentation.html#design_replicatedlog
倾向于保证新加入的一致，可能丢失部分

比起少数服从多数 所需节点大大减少

## 副本集合 分类

AR assigned Replica 所有
ISR In-sync Replica 
OSR Out-of-sync Replica

AR = ISR +OSR

## 副本因子 n
1 * leader + (n-1) * replica
## leader
AR 第一个Replica prefferPrelica
启动时会设置成 leader

leader 副本负责读写
leader 副本负责 维护和跟踪 ISR 集合中 follower 副本的滞后状态

## ISR
### 定时任务
isr-expiration
isr-change-propogation
周期： replica.lag.time.max.ms 一半
isr记录位置： /broker/topics/<topic>/partition/<partition>/state
### 缓存 isrChangeSet 减少频繁通知
缓存： isrChangeSet
isr变更记录位置： /isr_change_notification/isr_change_xxxxxx
条件：
 - last change > 5s
 - last write zookeeper > 60s

### follower变化 

收缩 有失效副本
扩展 追赶上leader 的 HW， 即 ISR 最小LEO

ISR 变化 也可能影响 分区HW
ISR 中的foller 才可能被选为leader


### leader 变化
#### 问题v0.11.0 
leader切换， hw 可能变了 导致
数据丢失 小挂大挂，leader选了小的HW 截断了HW到LEO之间的，丢了
follow 副本数据不一致 同时挂
#### leader epoch介入
截断数据的时候 使用 leader epoch作为参考 而不是 HW

副本通过 [[副本选举]] 成为leader 后 更新 <leaderEpoch，startoffset> startoffset 是 之前的leo

每个副本 记录在内存 和 leader-epoch-checkpoint 文件 （log）

v2 消息体中也有字段 partition leader epoch

##### 步骤
恢复后 先获取 当前 epoch offset
发送 OffsetForLeaderEpochRequest


